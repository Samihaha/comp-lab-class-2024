                      :-) GROMACS - gmx grompp, 2023.3 (-:

Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step4.0_minimization.mdp -o step4.0_minimization.tpr -c step3_input.gro -r step3_input.gro -p topol.top -n index.ndx -maxwarn 2

Generating 1-4 interactions: fudge = 0.5
Number of degrees of freedom in T-Coupling group rest is 24784.00
The integrator does not provide a ensemble temperature, there is no system ensemble temperature

NOTE 1 [file step4.0_minimization.mdp]:
  Removing center of mass motion in the presence of position restraints
  might cause artifacts. When you are using position restraints to
  equilibrate a macro-molecule, the artifacts are usually negligible.


There was 1 NOTE

Back Off! I just backed up step4.0_minimization.tpr to ./#step4.0_minimization.tpr.3#

GROMACS reminds you: "In mathematics you don't understand things, you just get used to them" (John von Neumann)

Setting the LD random seed to -1447429

Generated 1035 of the 1035 non-bonded parameter combinations

Generated 1035 of the 1035 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'PROA'

turning H bonds into constraints...

Excluding 1 bonded neighbours molecule type 'K+'

turning H bonds into constraints...

Excluding 1 bonded neighbours molecule type 'Cl-'

turning H bonds into constraints...

Excluding 1 bonded neighbours molecule type 'OPC'

turning H bonds into constraints...

Cleaning up constraints and constant bonded interactions with virtual sites

The largest distance between excluded atoms is 0.405 nm between atom 230 and 238
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 44x44x44, spacing 0.116 0.116 0.116

Estimate for the relative computational load of the PME mesh part: 0.23

This run will generate roughly 1 Mb of data
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 359602 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.1. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 1: Out Of Memory
srun: Terminating StepId=51673031.1
slurmstepd: error: *** STEP 51673031.1 ON cm017 CANCELLED AT 2024-10-03T14:54:21 ***
                      :-) GROMACS - gmx grompp, 2023.3 (-:

Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step4.1_equilibration.mdp -o step4.1_equilibration.tpr -c step4.0_minimization.gro -r step3_input.gro -p topol.top -n index.ndx -maxwarn 2

Generating 1-4 interactions: fudge = 0.5
Number of degrees of freedom in T-Coupling group SOLU is 1258.00
Number of degrees of freedom in T-Coupling group SOLV is 23523.00

NOTE 1 [file step4.1_equilibration.mdp]:
  Removing center of mass motion in the presence of position restraints
  might cause artifacts. When you are using position restraints to
  equilibrate a macro-molecule, the artifacts are usually negligible.


There was 1 NOTE

Back Off! I just backed up step4.1_equilibration.tpr to ./#step4.1_equilibration.tpr.2#

GROMACS reminds you: "Hmm, It *Does* Go Well With the Chicken" (Beastie Boys)

Setting the LD random seed to 2004876473

Generated 1035 of the 1035 non-bonded parameter combinations

Generated 1035 of the 1035 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'PROA'

turning H bonds into constraints...

Excluding 1 bonded neighbours molecule type 'K+'

turning H bonds into constraints...

Excluding 1 bonded neighbours molecule type 'Cl-'

turning H bonds into constraints...

Excluding 1 bonded neighbours molecule type 'OPC'

turning H bonds into constraints...

Setting gen_seed to -3416835

Velocities were taken from a Maxwell distribution at 310 K

Cleaning up constraints and constant bonded interactions with virtual sites

The largest distance between excluded atoms is 0.404 nm between atom 230 and 238

Determining Verlet buffer for a tolerance of 0.005 kJ/mol/ps at 310 K

Calculated rlist for 1x1 atom pair-list as 0.949 nm, buffer size 0.049 nm

Set rlist, assuming 4x4 atom pair-list, to 0.913 nm, buffer size 0.013 nm

Note that mdrun will redetermine rlist based on the actual pair-list setup
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 44x44x44, spacing 0.116 0.116 0.116

Estimate for the relative computational load of the PME mesh part: 0.24

This run will generate roughly 4 Mb of data
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 361610 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.3. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 2: Out Of Memory
srun: Terminating StepId=51673031.3
slurmstepd: error: *** STEP 51673031.3 ON cm017 CANCELLED AT 2024-10-03T14:54:30 ***
                      :-) GROMACS - gmx grompp, 2023.3 (-:

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step5_production.mdp -o step5_1.tpr -c step4.1_equilibration.gro -p topol.top -n index.ndx


-------------------------------------------------------
Program:     gmx grompp, version 2023.3
Source file: src/gromacs/commandline/cmdlineparser.cpp (line 271)
Function:    void gmx::CommandLineParser::parse(int *, char **)

Error in user input:
Invalid command-line options
  In command-line option -c
    File 'step4.1_equilibration.gro' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
In: PMI_Abort(1, N/A)
slurmstepd: error: *** STEP 51673031.4 ON cm017 CANCELLED AT 2024-10-03T14:54:32 ***
srun: error: cm017: task 0: Killed
srun: Terminating StepId=51673031.4
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 363335 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.5. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 5: Out Of Memory
srun: Terminating StepId=51673031.5
slurmstepd: error: *** STEP 51673031.5 ON cm017 CANCELLED AT 2024-10-03T14:54:39 ***
                      :-) GROMACS - gmx grompp, 2023.3 (-:

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step5_production.mdp -o step5_2.tpr -c step5_1.gro -t step5_1.cpt -p topol.top -n index.ndx


-------------------------------------------------------
Program:     gmx grompp, version 2023.3
Source file: src/gromacs/commandline/cmdlineparser.cpp (line 271)
Function:    void gmx::CommandLineParser::parse(int *, char **)

Error in user input:
Invalid command-line options
  In command-line option -c
    File 'step5_1.gro' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)
  In command-line option -t
    File 'step5_1.cpt' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
In: PMI_Abort(1, N/A)
slurmstepd: error: *** STEP 51673031.6 ON cm017 CANCELLED AT 2024-10-03T14:54:41 ***
srun: error: cm017: task 0: Killed
srun: Terminating StepId=51673031.6
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 365396 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.7. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 1: Out Of Memory
srun: Terminating StepId=51673031.7
slurmstepd: error: *** STEP 51673031.7 ON cm017 CANCELLED AT 2024-10-03T14:54:48 ***
                      :-) GROMACS - gmx grompp, 2023.3 (-:

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step5_production.mdp -o step5_3.tpr -c step5_2.gro -t step5_2.cpt -p topol.top -n index.ndx


-------------------------------------------------------
Program:     gmx grompp, version 2023.3
Source file: src/gromacs/commandline/cmdlineparser.cpp (line 271)
Function:    void gmx::CommandLineParser::parse(int *, char **)

Error in user input:
Invalid command-line options
  In command-line option -c
    File 'step5_2.gro' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)
  In command-line option -t
    File 'step5_2.cpt' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
In: PMI_Abort(1, N/A)
slurmstepd: error: *** STEP 51673031.8 ON cm017 CANCELLED AT 2024-10-03T14:54:50 ***
srun: error: cm017: task 0: Killed
srun: Terminating StepId=51673031.8
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 367212 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.9. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 3: Out Of Memory
srun: Terminating StepId=51673031.9
slurmstepd: error: *** STEP 51673031.9 ON cm017 CANCELLED AT 2024-10-03T14:54:57 ***
                      :-) GROMACS - gmx grompp, 2023.3 (-:

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step5_production.mdp -o step5_4.tpr -c step5_3.gro -t step5_3.cpt -p topol.top -n index.ndx


-------------------------------------------------------
Program:     gmx grompp, version 2023.3
Source file: src/gromacs/commandline/cmdlineparser.cpp (line 271)
Function:    void gmx::CommandLineParser::parse(int *, char **)

Error in user input:
Invalid command-line options
  In command-line option -c
    File 'step5_3.gro' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)
  In command-line option -t
    File 'step5_3.cpt' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
In: PMI_Abort(1, N/A)
slurmstepd: error: *** STEP 51673031.10 ON cm017 CANCELLED AT 2024-10-03T14:54:59 ***
srun: error: cm017: task 0: Killed
srun: Terminating StepId=51673031.10
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 368983 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.11. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 1: Out Of Memory
srun: Terminating StepId=51673031.11
slurmstepd: error: *** STEP 51673031.11 ON cm017 CANCELLED AT 2024-10-03T14:55:05 ***
                      :-) GROMACS - gmx grompp, 2023.3 (-:

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
Executable:   /ext3/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi
Data prefix:  /ext3/apps/gromacs/2023.3/openmpi/intel
Working dir:  /scratch/work/courses/CHEM-GA-2671-2024fa/students/sa8200/comp-lab-class-2024/Week4-CharmmGuiSetup/Setup/ArtEst_AMBER_OPC/gromacs
Command line:
  gmx_mpi grompp -f step5_production.mdp -o step5_5.tpr -c step5_4.gro -t step5_4.cpt -p topol.top -n index.ndx


-------------------------------------------------------
Program:     gmx grompp, version 2023.3
Source file: src/gromacs/commandline/cmdlineparser.cpp (line 271)
Function:    void gmx::CommandLineParser::parse(int *, char **)

Error in user input:
Invalid command-line options
  In command-line option -c
    File 'step5_4.gro' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)
  In command-line option -t
    File 'step5_4.cpt' does not exist or is not accessible.
    The file could not be opened.
      Reason: No such file or directory
      (call to fopen() returned error code 2)

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
In: PMI_Abort(1, N/A)
slurmstepd: error: *** STEP 51673031.12 ON cm017 CANCELLED AT 2024-10-03T14:55:08 ***
srun: error: cm017: task 0: Killed
srun: Terminating StepId=51673031.12
/share/apps/gromacs/2023.3/openmpi/intel/bin/gmx_mpi: line 31: 370951 Killed                  singularity exec --overlay /scratch/work/public/singularity/gromacs-2023.3.sqf:ro --overlay /scratch/work/public/singularity/intel-oneapi-2022.1.2.sqf:ro --overlay /scratch/work/public/singularity/openmpi-4.1.4-ubuntu-20.04.4-intel-2022.1.2.sqf:ro /scratch/work/public/singularity/ubuntu-22.04.3.sif /bin/bash -c "
unset -f which
if [[ -e /ext3/apps/openmpi-env.sh ]]; then source /ext3/apps/openmpi-env.sh; fi
if [[ -e /ext3/env.sh ]]; then source /ext3/env.sh; fi
module load gromacs/openmpi/intel/2023.3
${cmd} ${args}
"
slurmstepd: error: Detected 1 oom_kill event in StepId=51673031.13. Some of the step tasks have been OOM Killed.
srun: error: cm017: task 4: Out Of Memory
srun: Terminating StepId=51673031.13
slurmstepd: error: *** STEP 51673031.13 ON cm017 CANCELLED AT 2024-10-03T14:55:14 ***
